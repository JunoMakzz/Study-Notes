# Zookeeper快速领导者选举原理

## Zookeeper选举的基本原理

Zookeeper集群模式下才需要选举，在三种情况下需要进行选举：

**（1）集群启动的时候   （2）Leader挂掉的时候  （3）Leader的Follower者挂掉后没有过半的Follower**

Zookeeper的选举和人类的选举逻辑类似，Zookeeper需要实现四个基本概念；

1. 个人能力：Zookeeper是一个数据库，集群中节点的数据越新就代表此节点能力越强，而在Zookeeper中可以通**事务id(zxid)来表示数据的新旧**，一个节点最新的zxid越大则该节点的数据越新。所以**Zookeeper选举时会根据zxid的大小来作为投票的基本规则**。
2. 改票：Zookeeper集群中的某一个节点在开始进行选举时，首先认为自己的数据是最新的，会先投自己一票，并且把这张选票发送给其他服务器，这张选票里包含了两个重要信息：**zxid**和**sid**，sid表示这张选票投的服务器id，zxid表示这张选票投的服务器上最新的事务id，同时也会接收到其他服务器的选票，接收到其他服务器的选票后，可以根据选票信息中的zxid来与自己当前所投的服务器上的最大zxid来进行比较，**如果其他服务器的选票中的zxid较大，则表示自己当前所投的机器数据没有接收到的选票所投的服务器上的数据新，所以本节点需要改票，改成投给和刚刚接收到的选票一样，如果zxid一样的话就比较sid。**==如果在一个稳定的集群里添加一个新的节点，并且节点的zxid比其他节点都要大，这时zookeeper会把新节点的zxid回滚（比较大的时候）或者更新（比较小的时候）到与集群里的节点的zxid一致。==
3. 投票箱：Zookeeper集群中会有很多节点，和人类选举不一样，Zookeeper集群并不会单独去维护一个投票箱应用，而是**在每个节点内存里利用一个数组来作为投票箱**。**每个节点里都有一个投票箱，节点会将自己的选票以及从其他服务器接收到的选票放在这个投票箱中**。因为集群节点是相互交互的，并且选票的PK规则是一致的，所以每个节点里的这个投票箱所存储的选票都会是一样的，这样也可以达到公用一个投票箱的目的。
4. 领导者：Zookeeper集群中的每个节点，开始进行领导选举后，会不断的接收其他节点的选票，然后进行选票PK，将自己的选票修改为投给数据最新的节点，这样就保证了，每个节点自己的选票代表的都是自己暂时所认为的数据最新的节点，再因为其他服务器的选票都会存储在投票箱内，所以可以根据投票箱里去统计是否有超过一半的选票和自己选择的是同一个节点，都认为这个节点的数据最新，**一旦整个集群里超过一半的节点都认为某一个节点上的数据最新，则该节点就是领导者。**

通过对四个概念的在Zookeeper中的解析，也同时介绍了一下Zookeeper领导者选举的基本原理，只是说选举过程中还有更多的细节需要我们了解，下面我结合源码来给大家详细的分析一下Zookeeper的快速领导者选举原理。

## 领导者选举入口

ZooKeeperServer表示单机模式中的一个zkServer。

QuoruPeer表示集群模式中的一个zkServer。

QuoruPeer类定义如下：

```java
public class QuorumPeer extends ZooKeeperThread implements QuorumStats.Provider
```

定义表明QuorumPeer是一个ZooKeeperThread，表示是一个线程。

当集群中的某一个台zkServer启动时QuorumPeer类的start方法将被调用。

```java
public synchronized void start() {
        loadDataBase(); // 1
        cnxnFactory.start(); // 2
        startLeaderElection(); // 3 
        super.start();  // 4
    }
```

1. zkServer中有一个内存数据库对象ZKDatabase， zkServer在启动时需要将已被持久化的数据加载进内存中，也就是加载至ZKDatabase。
2. 这一步会开启一个线程来接收客户端请求，但是需要注意，这一步执行完后虽然成功开启了一个线程，但是因为现在zkServer还没有经过初始化，实际上把请求拒绝掉，直到zkServer初始化完成才能正常的接收请求。
3. 这个方法名很有误导性，这个方法并没有真正的开始领导选举，而是进行一些初始化
4. 继续启动，包括进行领导者选举、zkServer初始化。

## 领导者选举策略

上文QuorumPeer类的startLeaderElection会进行领导者选举初始化。

首先，领导者选举在Zookeeper中有3种实现：

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201104213452.png)

其中LeaderElection、AuthFastLeaderElection已经被标为过期，不建议使用，所以现在用的都是**快速领导者选举FastLeaderElection**，我们着重来介绍FastLeaderElection。

## 快速领导者选举

快速领导者选举实现架构如下图：

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201104213452.png)

### 传输层初始化

从架构图我们可以发现，快速领导者选举实现架构分为两层：**应用层**和**传输层**。所以初始化核心就是初始化传输层。

初始化步骤：

1. 初始化QuorumCnxManager
2. 初始化QuorumCnxManager.Listener
3. 运行QuorumCnxManager.Listener
4. 运行QuorumCnxManager
5. 返回FastLeaderElection对象

#### QuorumCnxManager介绍

QuorumCnxManager就是传输层实现，QuorumCnxManager中几个重要的属性：

- ConcurrentHashMap<Long, ArrayBlockingQueue<ByteBuffer>> **queueSendMap**
- ConcurrentHashMap<Long, SendWorker> **senderWorkerMap**
- ArrayBlockingQueue<Message> **recvQueue**
- QuorumCnxManager.Listener

传输层的每个zkServer需要发送选票信息给其他服务器，这些选票信息来至应用层，在传输层中将会按服务器id分组保存在**queueSendMap**中。

传输层的每个zkServer需要发送选票信息给其他服务器，SendWorker就是封装了Socket的发送器，而**senderWorkerMap**就是用来记录其他服务器id以及对应的SendWorker的。

传输层的每个zkServer将接收其他服务器发送的选票信息，这些选票会保存在**recvQueue**中，以提供给应用层使用。

QuorumCnxManager.Listener负责开启socket监听。

细化后的架构图如下：

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201104213501.png)

#### 服务器之间连接问题

在集群启动时，一台服务器需要去连另外一台服务器，从而建立Socket用来进行选票传输。那么如果现在A服务器去连B服务器，同时B服务器也去连A服务器，那么就会导致建立了两条Socket，我们知道Socket是双向的，Socket的双方是可以相互发送和接收数据的，那么现在A、B两台服务器建立两条Socket是没有意义的，所以ZooKeeper在实现时做了限制，只允许**服务器ID较大者去连服务器ID较小者，小ID服务器去连大ID服务器会被拒绝**，伪代码如下**：**

```java
if (对方服务器id < 本服务器id) {
    closeSocket(sock); // 关闭这条socket
    connectOne(sid);   // 由本服务器去连对方服务器
} else {
    // 继续建立连接
}
```

#### SendWorker、RecvWorker介绍

上文介绍到了SendWorker，它是zkServer用来向其他服务器发送选票信息的。

类结构如下：

```java
class SendWorker extends ZooKeeperThread {
    Long sid;
    Socket sock;
    RecvWorker recvWorker;
    volatile boolean running = true;
    DataOutputStream dout;
}
```

它封装了socket并且是一个线程，实际上SendWorker的底层实现是：SendWorker线程会不停的从**queueSendMap**中获取选票信息然后发送到Socket上。

基于同样的思路，我们还需要一个线程从Socket上获取数据然后添加到**recvQueue**中，这就是**RecvWorker**的功能。

所以架构可以演化为下图，通过这个架构，选举应用层直接从recvQueue中获取选票，或者选票添加到queueSendMap中即可以完成选票发送：

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201104213504.png)

### 应用层初始化

#### FastLeaderElection类介绍

FastLeaderElection类是快速领导者选举实现的核心类，这个类有三个重要的属性：

- LinkedBlockingQueue<**ToSend**> **sendqueue**;
- LinkedBlockingQueue<**Notification**> **recvqueue**;
- Messenger **messenger**;

- - Messenger.WorkerSender
  - Messenger.WorkerReceiver

服务器在进行领导者选举时，在发送选票时也会同时接受其他服务器的选票，FastLeaderElection类也提供了和传输层类似的实现，将待发送的选票放在**sendqueue**中，由**Messenger.WorkerSender**发送到传输层**queueSendMap**中。

同样，由**Messenger.WorkerReceiver**负责从传输层获取数据并放入**recvqueue**中。

这样在应用层只需要将待发送的选票信息添加到**sendqueue**中即可完成选票信息发送，或者从**recvqueue**中获取元素即可得到选票信息。

在构造FastLeaderElection对象时，会对**sendqueue、recvqueue**队列进行初始化，并且运行**Messenger.WorkerSender**与**Messenger.WorkerReceiver**线程。

此时架构图如下：

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201104213508.png)

到这里，QuorumPeer类的startLeaderElection方法已经执行完成，完成了传输层和应用层的初始化。

### 快速领导者选举实现

QuorumPeer类的start方法前三步分析完，接下来我们来看看第四步：

```java
super.start();
```

QuorumPeer类是一个ZooKeeperThread线程，上述代码实际就是运行一个线程，相当于运行QuorumPeer类中的run方法，这个方法也是集群模式下Zkserver启动最核心的方法。

总结一下QuorumPeer类的start方法：

1. 加载持久化数据到内存
2. 初始化领导者选举策略
3. 初始化快速领导者选举传输层
4. 初始化快速领导者选举应用层
5. 开启主线程

主线程开启之后，QuorumPeer类的start方法即执行完成，这时回到上层代码可以看到主线程会被join住：

```java
quorumPeer.start(); // 开启线程
quorumPeer.join(); // join线程
```

接下来我们着重来分析一下主线程内的逻辑。

#### 主线程

在主线程里，会有一个主循环(Main loop)，主循环伪代码如下：

```java
while (服务是否正在运行) {
    switch (当前服务器状态) {
        case LOOKING:
            // 领导者选举
            setCurrentVote(makeLEStrategy().lookForLeader());
            break;
        case OBSERVING:
            try {
                // 初始化为观察者
            } catch (Exception e) {
                LOG.warn("Unexpected exception",e );                        
            } finally {
                observer.shutdown();
                setPeerState(ServerState.LOOKING);
            }
            break;
        case FOLLOWING:
            try {
                // 初始化为跟随者
            } catch (Exception e) {
                LOG.warn("Unexpected exception",e);
            } finally {
                follower.shutdown();
                setPeerState(ServerState.LOOKING);
            }
            break;
        case LEADING:
            try {
                // 初始化为领导者
            } catch (Exception e) {
                LOG.warn("Unexpected exception",e);
            } finally {
                leader.shutdown("Forcing shutdown");
                setPeerState(ServerState.LOOKING);
            }
        break;
    }
}
```

这个伪代码实际上**非常非常重要，大家细心的多看几遍。**

根据伪代码可以看到，当服务器状态为LOOKING时会进行领导者选举，所以我们着重来看领导者选举。

#### lookForLeader

当服务器状态为LOOKING时会调用FastLeaderElection类的lookForLeader方法，这就是领导者选举的应用层。

##### 1.初始化一个投票箱

```java
HashMap<Long, Vote> recvset = new HashMap<Long, Vote>();
```

##### 2.更新选票，将票投给自己

```java
updateProposal(getInitId(), getInitLastLoggedZxid(), getPeerEpoch());
```

##### 3.发送选票

```java
sendNotifications();
```

##### 4.不断获取其他服务器的投票信息，直到选出Leader

```java
while ((self.getPeerState() == ServerState.LOOKING) && (!stop)){
    // 从recvqueue中获取接收到的投票信息
    Notification n = recvqueue.poll(notTimeout, TimeUnit.MILLISECONDS);
    
    if (获得的投票为空) {
        // 连接其他服务器
    } else {
        // 处理投票
    }
}
```

##### 5.连接其他服务器

因为在这一步之前，都只进行了服务器的初始化，并没有真正的去与其他服务器建立连接，所以在这里建立连接。

##### 6.处理投票

判断接收到的投票所对应的服务器的状态，也就是投此票的服务器的状态：

```java
switch (n.state) {
    case LOOKING:
        // PK选票、过半机制验证等
        break;
    case OBSERVING:
        // 观察者节点不应该发起投票，直接忽略
        break;
    case FOLLOWING:
    case LEADING:
        // 如果接收到跟随者或领导者节点的选票，则可以认为当前集群已经存在Leader了，直接return，退出lookForLeader方法。
}
```

##### 7. PK选票

```java
if (接收到的投票的选举周期 > 本服务器当前的选举周期) {
    // 修改本服务器的选举周期为接收到的投票的选举周期
    // 清空本服务器的投票箱（表示选举周期落后，重新开始投票）
    // 比较接收到的选票所选择的服务器与本服务器的数据谁更新，本服务器将选票投给数据较新者
    // 发送选票
} else if(接收到的投票的选举周期 < 本服务器当前的选举周期){
    // 接收到的投票的选举周期落后了，本服务器直接忽略此投票
} else if(选举周期一致) {
    // 比较接收到的选票所选择的服务器与本服务器当前所选择的服务器的数据谁更新，本服务器将选票投给数据较新者
    // 发送选票
}
```

##### 8.过半机制验证

本服务器的选票经过不停的PK会将票投给数据更新的服务器，PK完后，将接收到的选票以及本服务器自己所投的选票放入投票箱中，然后从投票箱中统计出与本服务器当前所投服务器一致的选票数量，判断该选票数量是否超过集群中所有跟随者的一半（选票数量 > 跟随者数量/2），如果满足这个过半机制就选出了一个**准Leader。**

##### 9.最终确认

选出**准Leader**之后，再去获取其他服务器的选票，如果获取到的选票所代表的服务器的数据比准Leader更新，则准Leader卸职，继续选举。如果没有准Leader更新，则继续获取投票，直到没有获取到选票，则选出了最终的Leader。

Leader确定后，其他服务器的角色也确定好了。

## 领导选举完成后

上文**主线程小节**有一段非常重要的伪代码，这段伪代码达到了一个非常重要的功能，就是：

> **ZooKeeper集群在进行领导者选举的过程中不能对外提供服务**

根据伪代码我们可以发现，只有当集群中服务器的角色确定了之后，while才会进行下一次循环，当进入下一次循环后，就会根据服务器的角色进入到对应的初始化逻辑，初始化完成之后才能对外提供服务。



# Zookeeper请求处理原理分析

Zookeeper是可以存储数据的，所以我们可以把它理解一个数据库，实际上它的底层原理本身也和数据库是类似的。

## 数据库的原理

我们知道，数据库是用来存储数据的，只是数据可以存储在内存中或磁盘中。而Zookeeper实际是结合了这两种的，Zookeeper中的数据即会存储在磁盘中以达到持久化的目的，也会同步到内存中以到达快速访问的目的。

事实上，用过Zookeeper的同学应该知道，Zookeeper中有两种类型的节点：**持久化节点**和**临时节点**。

- 持久化节点：会持久化在磁盘中，除非主动删除，将一直存在。
- 临时节点：不会持久化在磁盘中，只会存储在内存中，创建这个临时节点的Session一旦过期，此临时节点也将自动被删除。

## 数据库处理数据的原理

作为一个数据库，肯定是要接收客户端创建、修改、删除、查询节点等请求的。

在Zookeeper中对于请求分为两类：

- 事务性请求
- 非事务性请求

### 事务性请求

Zookeeper通常都是以集群模式运行的，也就是Zookeeper集群中各个节点的数据需要保持一致的。但是和Mysql集群不一样的是：

- Mysql集群中，从服务器是异步从主服务器同步数据的，这中间的间隔时间可以比较长。
- Zookeeper集群中，当某一个集群节点接收到一个写请求操作时，该节点需要将这个写请求操作发送给其他节点，以使其他节点同步执行这个写请求操作，从而达到各个节点上的数据保持一致，也就是数据一致性。我们通常说Zookeeper保证CAP理论中的CP就是这个意思。

> Zookeeper集群底层是怎么保证数据一致性的，其实是用的**两阶段提交+过半机制**来保证的

事务性请求包括：更新操作、新增操作、删除操作，结合上面的分析，因为这些操作是会影响数据的，所以要保证这些操作在整个集群内的事务性，所以这些操作就是事务性请求。

### 非事务性请求

那么非事务性请求就好理解的，像查询操作、exist操作这些不影响数据的操场，就不需要集群来保持事务性，所以这些操场就是非事务性请求。

> Zookeeper在处理事务性请求时，比处理非事务性请求要复杂很多

## 数据在磁盘中的表示

假设我们现在在Zookeeper中有一个数据节点，节点名为`/datanode`，内容为`125`，该节点是持久化节点，所以该节点信息会保存在文件中。

可能大家都会认为是类似下面这样方式保存在磁盘文件中的，方法一：

| 节点名    | 节点内容 |
| --------- | -------- |
| /datanode | 125      |

但是除开这种表示方法，还有另外一种表示方法，**快照+事务日志**，比如方法二：

当前快照：

| 节点名    | 节点内容 |
| --------- | -------- |
| /datanode | 120      |

当前事务日志：

| 事务ID  | 操作   | 节点名    | 节点内容修改前 | 节点内容修改后 |
| ------- | ------ | --------- | -------------- | -------------- |
| 1000010 | update | /datanode | 120            | 121            |
| 1000011 | update | /datanode | 121            | 125            |

乍一看方法二比方法一要更复杂，并且占用的磁盘更多。但是我们上文提到过，Zookeeper集群中的节点在处理事务性请求时，需要将**事务操作**同步给其他节点，所以这里的事务操作是一定要进行持久化的，以便在同步给其他节点时出现异常进行补偿。所以就出现了**事务日志**。实际上事务日志还运行数据进行**回滚**，这个在两阶段提交中也是非常重要的。

那么**快照**又有什么用呢？事务日志一定要有，但是随着时间的推移，日志肯定会越来越多，所以肯定不能持久化历史上所有的日志，所以Zookeeper会定时的进行**快照**，并删除之前的日志。

那么如果按方法二这么存储数据，在对数据进行查询时就不太方便了。上文说到，Zookeeper为了提高数据的查询速度，会在内存中也存储一份数据，那么内存中的这份数据又该怎么存呢？

## 数据在内存中的表示

Zookeeper中的数据在内存中的表示其实和上文的方法一很类似，只是Zookeeper中的数据是具有文件目录特点的，说白了就是Zookeeper中的数据节点的名字一定要以`“/”`开头，这样就导致Zookeeper中的数据类似一颗树：

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201104225658.png)

一颗具有父子层级的多叉树，在Zookeeper源码中叫**DataTree**。

## 请求处理逻辑

请看下图：**事务性请求都是在Leader上进行操作的，即使请求是发给Follower节点，Follower节点也会把请求转发给Leader节点**

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201104225708.png)

> 请注意，对于上图，Zookeeper真正的底层实现，zk1是Leader，zk2和zk3是Learner，是根据[领导者选举](https://mp.weixin.qq.com/s/z73f6rQXYvh2byfkO8tMoA)选出来的。

**非事务性请求直接读取收到请求的节点上DataTree上的内容，DataTree是在内存中的，所以会非常快。**

## 总结

这篇文章介绍了Zookeeper在处理请求时的几个核心概念：

1. 事务性请求
2. 事务日志
3. 快照
4. DataTree
5. 两阶段提交

![image-20201107215632391](https://gitee.com/xudongyin/img/raw/master/img/20201107215633.png)

为了提高读请求的吞吐量，最好添加的是observer节点，原因是：

**因为添加一个Follower节点的话虽然也可以提高读请求的吞吐量，但是会降低写请求的处理速度，因为写请求需要对Follower节点进行数据同步，也就是二次提交，Follower节点太多不好。如果添加的是observer节点的话，每次写请求后进行数据同步都只需提交一次，不需要进行二次提交。**

# Zookeeper如何解决脑裂问题

## 什么是脑裂

脑裂(split-brain)就是“大脑分裂”，也就是本来一个“大脑”被拆分了两个或多个“大脑”，我们都知道，如果一个人有多个大脑，并且相互独立的话，那么会导致人体“手舞足蹈”，“不听使唤”。

脑裂通常会出现在集群环境中，比如ElasticSearch、Zookeeper集群，而这些集群环境有一个统一的特点，就是它们有一个大脑，比如ElasticSearch集群中有Master节点，Zookeeper集群中有Leader节点。

## Zookeeper集群中的脑裂场景

对于一个集群，想要提高这个集群的可用性，通常会采用多机房部署，比如现在有一个由6台zkServer所组成的一个集群，部署在了两个机房：

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201104225743.png)

正常情况下，此集群只会有一个Leader，那么如果机房之间的网络断了之后，两个机房内的zkServer还是可以相互通信的，如果**不考虑过半机制**，那么就会出现每个机房内部都将选出一个Leader。

**![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201104225750.png)**

这就相当于原本一个集群，被分成了两个集群，出现了两个“大脑”，这就是脑裂。

对于这种情况，我们也可以看出来，原本应该是统一的一个集群对外提供服务的，现在变成了两个集群同时对外提供服务，如果过了一会，断了的网络突然联通了，那么此时就会出现问题了，两个集群刚刚都对外提供服务了，数据该怎么合并，数据冲突怎么解决等等问题。

刚刚在说明脑裂场景时，有一个前提条件就是没有考虑过半机制，所以**实际上Zookeeper集群中是不会出现脑裂问题的，而不会出现的原因就跟过半机制有关。**

## 过半机制

在领导者选举的过程中，如果某台zkServer获得了超过半数的选票，则此zkServer就可以成为Leader了。

过半机制的源码实现其实非常简单：

```java
public class QuorumMaj implements QuorumVerifier {
    private static final Logger LOG = LoggerFactory.getLogger(QuorumMaj.class);
    
    int half;
    
    // n表示集群中zkServer的个数（准确的说是参与者的个数，参与者不包括观察者节点）
    public QuorumMaj(int n){
        this.half = n/2;
    }

    // 验证是否符合过半机制
    public boolean containsQuorum(Set<Long> set){
        // half是在构造方法里赋值的
        // set.size()表示某台zkServer获得的票数
        return (set.size() > half);
    }
    
}
```

大家仔细看一下上面方法中的注释，核心代码就是下面两行：

```java
this.half = n/2;
return (set.size() > half);
```

举个简单的例子：

如果现在集群中有5台zkServer，那么half=5/2=2，那么也就是说，领导者选举的过程中至少要有三台zkServer投了同一个zkServer，才会符合过半机制，才能选出来一个Leader。

那么有一个问题我们想一下，**选举的过程中为什么一定要有一个过半机制验证？**

因为这样不需要等待所有zkServer都投了同一个zkServer就可以选举出来一个Leader了，这样比较快，所以叫快速领导者选举算法呗。

那么再来想一个问题，**过半机制中为什么是大于，而不是大于等于呢？**

这就是跟脑裂问题有关系了，比如回到上文出现脑裂问题的场景：

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201104225823.png)

当机房中间的网络断掉之后，机房1内的三台服务器会进行领导者选举，但是此时过半机制的条件是set.size() > 3，也就是说至少要4台zkServer才能选出来一个Leader，所以对于机房1来说它不能选出一个Leader，同样机房2也不能选出一个Leader，这种情况下整个集群当机房间的网络断掉后，整个集群将没有Leader。

而如果过半机制的条件是set.size() >= 3，那么机房1和机房2都会选出一个Leader，这样就出现了脑裂。所以我们就知道了，为什么过半机制中是**大于**，而不是**大于等于**。就是为了防止脑裂。

如果假设我们现在只有5台机器，也部署在两个机房：

![image.png](https://cdn.nlark.com/yuque/0/2019/png/365147/1563865876119-268f52aa-3fce-4337-ab5a-ed0e19fb388c.png?x-oss-process=image%2Fwatermark%2Ctype_d3F5LW1pY3JvaGVp%2Csize_10%2Ctext_6bKB54-t5a2m6Zmi5ZGo55Gc%2Ccolor_FFFFFF%2Cshadow_50%2Ct_80%2Cg_se%2Cx_10%2Cy_10)

此时过半机制的条件是set.size() > 2，也就是至少要3台服务器才能选出一个Leader，此时机房件的网络断开了，对于机房1来说是没有影响的，Leader依然还是Leader，对于机房2来说是选不出来Leader的，此时整个集群中只有一个Leader。

所以，我们可以总结得出，有了过半机制，对于一个Zookeeper集群，要么没有Leader，要没只有1个Leader，这样就避免了脑裂问题。

# 分布式ID方案总结

ID是数据的唯一标识，传统的做法是利用UUID和数据库的自增ID，在互联网企业中，大部分公司使用的都是Mysql，并且因为需要事务支持，所以通常会使用Innodb存储引擎，UUID太长以及无序，所以并不适合在Innodb中来作为主键，自增ID比较合适，但是随着公司的业务发展，数据量将越来越大，需要对数据进行分表，而分表后，每个表中的数据都会按自己的节奏进行自增，很有可能出现ID冲突。这时就需要一个单独的机制来负责生成唯一ID，生成出来的ID也可以叫做**分布式ID**，或**全局ID**。下面来分析各个生成分布式ID的机制。

![img](https://gitee.com/xudongyin/img/raw/master/img/20201026133747.png)



## 数据库自增ID

第一种方案仍然还是基于数据库的自增ID，需要单独使用一个数据库实例，在这个实例中新建一个单独的表：

表结构如下：

```mysql
CREATE DATABASE `SEQID`;

CREATE TABLE SEQID.SEQUENCE_ID (
    id bigint(20) unsigned NOT NULL auto_increment, 
    stub char(10) NOT NULL default '',
    PRIMARY KEY (id),
    UNIQUE KEY stub (stub)
) ENGINE=MyISAM;
```

可以使用下面的语句生成并获取到一个自增ID

```mysql
begin;
replace into SEQUENCE_ID (stub) VALUES ('anyword');
select last_insert_id();
commit;
```

stub字段在这里并没有什么特殊的意义，只是为了方便的去插入数据，只有能插入数据才能产生自增id。而对于插入我们用的是replace，replace会先看是否存在stub指定值一样的数据，如果存在则先delete再insert，如果不存在则直接insert。

这种生成分布式ID的机制，需要一个单独的Mysql实例，虽然可行，但是基于性能与可靠性来考虑的话都不够，**业务系统每次需要一个ID时，都需要请求数据库获取，性能低，并且如果此数据库实例下线了，那么将影响所有的业务系统。**

为了解决数据库可靠性问题，我们可以使用第二种分布式ID生成方案。



## 数据库多主模式

如果我们两个数据库组成一个**主从模式**集群，正常情况下可以解决数据库可靠性问题，但是如果主库挂掉后，数据没有及时同步到从库，这个时候会出现ID重复的现象。我们可以使用**双主模式**集群，也就是两个Mysql实例都能单独的生产自增ID，这样能够提高效率，但是如果不经过其他改造的话，这两个Mysql实例很可能会生成同样的ID。需要单独给每个Mysql实例配置不同的起始值和自增步长。

第一台Mysql实例配置：

```mysql
set @@auto_increment_offset = 1;     -- 起始值
set @@auto_increment_increment = 2;  -- 步长
```

第二台Mysql实例配置：

```mysql
set @@auto_increment_offset = 2;     -- 起始值
set @@auto_increment_increment = 2;  -- 步长
```

经过上面的配置后，这两个Mysql实例生成的id序列如下：

mysql1,起始值为1,步长为2,ID生成的序列为：1,3,5,7,9,...

mysql2,起始值为2,步长为2,ID生成的序列为：2,4,6,8,10,...



对于这种生成分布式ID的方案，需要单独新增一个生成分布式ID应用，比如DistributIdService，该应用提供一个接口供业务应用获取ID，业务应用需要一个ID时，通过rpc的方式请求DistributIdService，DistributIdService随机去上面的两个Mysql实例中去获取ID。

实行这种方案后，就算其中某一台Mysql实例下线了，也不会影响DistributIdService，DistributIdService仍然可以利用另外一台Mysql来生成ID。

但是这种方案的扩展性不太好，如果两台Mysql实例不够用，需要新增Mysql实例来提高性能时，这时就会比较麻烦。

现在如果要新增一个实例mysql3，要怎么操作呢？

第一，mysql1、mysql2的步长肯定都要修改为3，而且只能是人工去修改，这是需要时间的。

第二，因为mysql1和mysql2是不停在自增的，对于mysql3的起始值我们可能要定得大一点，以给充分的时间去修改mysql1，mysql2的步长。

第三，在修改步长的时候很可能会出现重复ID，要解决这个问题，可能需要停机才行。

为了解决上面的问题，以及能够进一步提高DistributIdService的性能，建议使用第三种生成分布式ID机制。



## 号段模式

我们可以使用号段的方式来获取自增ID，号段可以理解成批量获取，比如DistributIdService从数据库获取ID时，如果能批量获取多个ID并缓存在本地的话，那样将大大提供业务应用获取ID的效率。

比如DistributIdService每次从数据库获取ID时，就获取一个号段，比如(1,1000]，这个范围表示了1000个ID，业务应用在请求DistributIdService提供ID时，DistributIdService只需要在本地从1开始自增并返回即可，而不需要每次都请求数据库，一直到本地自增到1000时，也就是当前号段已经被用完时，才去数据库重新获取下一号段。

所以，我们需要对数据库表进行改动，如下：

```mysql
CREATE TABLE id_generator (
  id int(10) NOT NULL,
  current_max_id bigint(20) NOT NULL COMMENT '当前最大id',
  increment_step int(10) NOT NULL COMMENT '号段的长度',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
```

这个数据库表用来记录自增步长以及当前自增ID的最大值（也就是当前已经被申请的号段的最后一个值），因为自增逻辑被移到DistributIdService中去了，所以数据库不需要这部分逻辑了。

这种方案不再强依赖数据库，就算数据库不可用，那么DistributIdService也能继续支撑一段时间。但是如果DistributIdService重启，会丢失一段ID，导致ID空洞。

为了提高DistributIdService的高可用，需要做一个集群，业务在请求DistributIdService集群获取ID时，会随机的选择某一个DistributIdService节点进行获取，对每一个DistributIdService节点来说，数据库连接的是同一个数据库，那么可能会产生多个DistributIdService节点同时请求数据库获取号段，那么这个时候需要利用乐观锁来进行控制，比如在数据库表中增加一个version字段，在获取号段时使用如下SQL：

```sql
update id_generator set current_max_id=#{newMaxId}, version=version+1 where version = #{version}
```

因为newMaxId是DistributIdService中根据oldMaxId+步长算出来的，只要上面的update更新成功了就表示号段获取成功了。

为了提供数据库层的高可用，需要对数据库使用多主模式进行部署，对于每个数据库来说要保证生成的号段不重复，这就需要利用最开始的思路，在刚刚的数据库表中增加起始值和步长，比如如果现在是两台Mysql，那么

mysql1将生成号段（1,1001]，自增的时候序列为1，3，4，5，7....

mysql1将生成号段（2,1002]，自增的时候序列为2，4，6，8，10...

更详细的可以参考滴滴开源的TinyId：

[项目源码](https://github.com/didi/tinyid/wiki/tinyid%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D)
[原理介绍](https://github.com/didi/tinyid/wiki/tinyid原理介绍)

在TinyId中还增加了一步来提高效率，在上面的实现中，ID自增的逻辑是在DistributIdService中实现的，而实际上可以把自增的逻辑转移到业务应用本地，这样对于业务应用来说只需要获取号段，每次自增时不再需要请求调用DistributIdService了。

**优化办法**

> 双号段缓存

 对于号段用完需要访问db，我们很容易想到在号段用到一定程度的时候，就去异步加载下一个号段，保证内存中始终有可用号段，则可避免性能波动。

> 增加多db支持

 db只有一个master时，如果db不可用(down掉或者主从延迟比较大)，则获取号段不可用。实际上我们可以支持多个db，比如2个db，A和B，我们获取号段可以随机从其中一台上获取。那么如果A,B都获取到了同一号段，我们怎么保证生成的id不重呢？tinyid是这么做的，让A只生成偶数id，B只生产奇数id，对应的db设计增加了两个字段，如下所示

| id   | biz_type | max_id | step | delta | remainder | version |
| ---- | -------- | ------ | ---- | ----- | --------- | ------- |
| 1    | 1000     | 2000   | 1000 | 2     | 0         | 0       |

 delta代表id每次的增量，remainder代表余数，例如可以将A，B都delta都设置2，remainder分别设置为0，1则，A的号段只生成偶数号段，B是奇数号段。 通过delta和remainder两个字段我们可以根据使用方的需求灵活设计db个数，同时也可以为使用方提供只生产类似奇数的id序列。

> 增加tinyid-client

 使用http获取一个id，存在网络开销，是否可以本地生成id？为此我们提供了tinyid-client，我们可以向tinyid-server发送请求来获取可用号段，之后在本地构建双号段、id生成，如此id生成则变成纯本地操作，性能大大提升，因为本地有双号段缓存，则可以容忍tinyid-server一段时间的down掉，可用性也有了比较大的提升。



## 雪花算法

上面的三种方法总的来说是基于自增思想的，而接下来就介绍比较著名的雪花算法-snowflake。

我们可以换个角度来对分布式ID进行思考，只要能让负责生成分布式ID的每台机器在每毫秒内生成不一样的ID就行了。

snowflake是twitter开源的分布式ID生成算法，是一种算法，所以它和上面的三种生成分布式ID机制不太一样，它不依赖数据库。

核心思想是：分布式ID固定是一个long型的数字，一个long型占8个字节，也就是64个bit，原始snowflake算法中对于bit的分配如下图：

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201026133758.png)

- 第一个bit位是标识部分，在java中由于long的最高位是符号位，正数是0，负数是1，一般生成的ID为正数，所以固定为0。
- 时间戳部分占41bit，这个是毫秒级的时间，一般实现上不会存储当前的时间戳，而是时间戳的差值（当前时间-固定的开始时间），这样可以使产生的ID从更小值开始；41位的时间戳可以使用69年，(1L << 41) / (1000L * 60 * 60 * 24 * 365) = 69年
- 工作机器id占10bit，这里比较灵活，比如，可以使用前5位作为数据中心机房标识，后5位作为单机房机器标识，可以部署1024个节点。
- 序列号部分占12bit，支持同一毫秒内同一个节点可以生成4096个ID

根据这个算法的逻辑，只需要将这个算法用Java语言实现出来，封装为一个工具方法，那么各个业务应用可以直接使用该工具方法来获取分布式ID，只需保证每个业务应用有自己的工作机器id即可，而不需要单独去搭建一个获取分布式ID的应用。

snowflake算法实现起来并不难，提供一个github上用java实现的：https://github.com/beyondfengyu/SnowFlake

在大厂里，其实并没有直接使用snowflake，而是进行了改造，因为snowflake算法中最难实践的就是工作机器id，原始的snowflake算法需要人工去为每台机器去指定一个机器id，并配置在某个地方从而让snowflake从此处获取机器id。

但是在大厂里，机器是很多的，人力成本太大且容易出错，所以大厂对snowflake进行了改造。

### 百度（uid-generator）

github地址：[uid-generator](https://github.com/baidu/uid-generator)

uid-generator使用的就是snowflake，只是在生产机器id，也叫做workId时有所不同。

uid-generator中的workId是由uid-generator自动生成的，并且考虑到了应用部署在docker上的情况，在uid-generator中用户可以自己去定义workId的生成策略，默认提供的策略是：应用启动时由数据库分配。说的简单一点就是：应用在启动时会往数据库表(uid-generator需要新增一个WORKER_NODE表)中去插入一条数据，数据插入成功后返回的该数据对应的自增唯一id就是该机器的workId，而数据由host，port组成。

对于uid-generator中的workId，占用了22个bit位，时间占用了28个bit位，序列化占用了13个bit位，需要注意的是，和原始的snowflake不太一样，时间的单位是秒，而不是毫秒，workId也不一样，同一个应用每重启一次就会消费一个workId。

具体可参考https://github.com/baidu/uid-generator/blob/master/README.zh_cn.md



### 美团（Leaf）

github地址：[Leaf](https://github.com/Meituan-Dianping/Leaf)

美团的Leaf也是一个分布式ID生成框架。它非常全面，即支持号段模式，也支持snowflake模式。号段模式这里就不介绍了，和上面的分析类似。

Leaf中的snowflake模式和原始snowflake算法的不同点，也主要在workId的生成，Leaf中workId是基于ZooKeeper的顺序Id来生成的，每个应用在使用Leaf-snowflake时，在启动时都会都在Zookeeper中生成一个顺序Id，相当于一台机器对应一个顺序节点，也就是一个workId。

### 总结

总得来说，上面两种都是自动生成workId，以让系统更加稳定以及减少人工操作。



## Redis

这里额外再介绍一下使用Redis来生成分布式ID，其实和利用Mysql自增ID类似，可以利用Redis中的incr命令来实现原子性的自增与返回，比如：

```bash
127.0.0.1:6379> set seq_id 1     // 初始化自增ID为1
OK
127.0.0.1:6379> incr seq_id      // 增加1，并返回
(integer) 2
127.0.0.1:6379> incr seq_id      // 增加1，并返回
(integer) 3
```

使用redis的效率是非常高的，但是要考虑持久化的问题。Redis支持RDB和AOF两种持久化的方式。

RDB持久化相当于定时打一个快照进行持久化，如果打完快照后，连续自增了几次，还没来得及做下一次快照持久化，这个时候Redis挂掉了，重启Redis后会出现ID重复。

AOF持久化相当于对每条写命令进行持久化，如果Redis挂掉了，不会出现ID重复的现象，但是会由于incr命令过多，导致重启恢复数据时间过长。

# 生产级负载均衡算法详解

## 负载均衡介绍

负载均衡，英文名称为Load Balance，指由多台服务器以对称的方式组成一个服务器集合，每台服务器都具有等价的地位，都可以单独对外提供服务而无须其他服务器的辅助。

通过某种负载分担技术，将外部发送来的请求均匀分配到对称结构中的某一台服务器上，而接收到请求的服务器独立地回应客户的请求。

负载均衡能够平均分配客户请求到服务器阵列，借此提供快速获取重要数据，解决大量并发访问服务问题，这种集群技术可以用最少的投资获得接近于大型主机的性能。

## 负载均衡方式

负载均衡分为软件负载均衡和硬件负载均衡

### 软件负载均衡

常见的负载均衡软件有Nginx、LVS、HAProxy。

关于这几个软件的特点比较不是本文重点，感兴趣同学可以参见博客：

- （总结）Nginx/LVS/HAProxy负载均衡软件的优缺点详解：http://www.ha97.com/5646.html
- 三大主流软件负载均衡器对比(LVS 、 Nginx 、Haproxy)：http://www.21yunwei.com/archives/5824

### 硬件负载均衡

常见的负载均衡硬件有Array、F5。

## 负载均衡算法

常见的负载均衡算法有：随机算法、加权轮询、一致性hash、最小活跃数算法。

**千万别以为这几个算法看上去都特别简单，但其实真正在生产上用到时会远比你想的复杂**

### 算法前提条件

定义一个服务器列表，每个负载均衡的算法会从中挑出一个服务器作为算法的结果。

```java
public class ServerIps {
    private static final List<String> LIST = Arrays.asList(
            "192.168.0.1",
            "192.168.0.2",
            "192.168.0.3",
            "192.168.0.4",
            "192.168.0.5",
            "192.168.0.6",
            "192.168.0.7",
            "192.168.0.8",
            "192.168.0.9",
            "192.168.0.10"
    );
}
```

### 随机算法-RandomLoadBalance

先来个最简单的实现。

```java
public class Random {
    public static String getServer() {
        // 生成一个随机数作为list的下标值
        java.util.Random random = new java.util.Random();
        int randomPos = random.nextInt(ServerIps.LIST.size());
        return ServerIps.LIST.get(randomPos);
    }
    public static void main(String[] args) {
        // 连续调用10次
        for (int i=0; i<10; i++) {
            System.out.println(getServer());
        }
    }
}
```

```bash
运行结果：
192.168.0.3
192.168.0.4
192.168.0.7
192.168.0.1
192.168.0.2
192.168.0.7
192.168.0.3
192.168.0.9
192.168.0.1
192.168.0.1
```

当调用次数比较少时，Random 产生的随机数可能会比较集中，此时多数请求会落到同一台服务器上，只有在经过多次请求后，才能使调用请求进行“均匀”分配。调用量少这一点并没有什么关系，负载均衡机制不正是为了应对请求量多的情况吗，所以随机算法也是用得比较多的一种算法。

> 但是，上面的随机算法适用于每天机器的性能差不多的时候，实际上，生产中可能某些机器的性能更高一点，它可以处理更多的请求，所以，我们可以对每台服务器设置一个权重。
>
> 在ServerIps类中增加服务器权重对应关系MAP，权重之和为50：

```java
public static final Map<String, Integer> WEIGHT_LIST = new HashMap<String, Integer>();
    static {
        // 权重之和为50
        WEIGHT_LIST.put("192.168.0.1", 1);
        WEIGHT_LIST.put("192.168.0.2", 8);
        WEIGHT_LIST.put("192.168.0.3", 3);
        WEIGHT_LIST.put("192.168.0.4", 6);
        WEIGHT_LIST.put("192.168.0.5", 5);
        WEIGHT_LIST.put("192.168.0.6", 5);
        WEIGHT_LIST.put("192.168.0.7", 4);
        WEIGHT_LIST.put("192.168.0.8", 7);
        WEIGHT_LIST.put("192.168.0.9", 2);
        WEIGHT_LIST.put("192.168.0.10", 9);
    }
```

那么现在的随机算法应该要改成**权重随机算法**，当调用量比较多的时候，服务器使用的分布应该近似对应权重的分布。

#### 权重随机算法

简单的实现思路是，把每个服务器按它所对应的服务器进行复制，具体看代码更加容易理解

```java
public class WeightRandom {
    public static String getServer() {
        // 生成一个随机数作为list的下标值
        List<String> ips = new ArrayList<String>();
        for (String ip : ServerIps.WEIGHT_LIST.keySet()) {
            Integer weight = ServerIps.WEIGHT_LIST.get(ip);
            // 按权重进行复制，本质还是随机算法，只是在随机的集合里面具有多个相同的值
            for (int i=0; i<weight; i++) {
                ips.add(ip);
            }
        }
        java.util.Random random = new java.util.Random();
        int randomPos = random.nextInt(ips.size());
        return ips.get(randomPos);
    }
    public static void main(String[] args) {
        // 连续调用10次
        for (int i=0; i<10; i++) {
            System.out.println(getServer());
        }
    }
}
```

```bash
运行结果：
192.168.0.8
192.168.0.2
192.168.0.7
192.168.0.10
192.168.0.8
192.168.0.8
192.168.0.4
192.168.0.7
192.168.0.6
192.168.0.8
```

**这种实现方法在遇到权重之和特别大的时候就会比较消耗内存，因为需要对ip地址进行复制，权重之和越大那么上文中的ips就需要越多的内存**，下面介绍另外一种实现思路。

假设我们有一组服务器 servers = [A, B, C]，他们对应的权重为 weights = [5, 3, 2]，权重总和为10。现在把这些权重值平铺在一维坐标值上，[0, 5) 区间属于服务器 A，[5, 8) 区间属于服务器 B，[8, 10) 区间属于服务器 C。接下来通过随机数生成器生成一个范围在 [0, 10) 之间的随机数，然后计算这个随机数会落到哪个区间上。比如数字3会落到服务器 A 对应的区间上，此时返回服务器 A 即可。权重越大的机器，在坐标轴上对应的区间范围就越大，因此随机数生成器生成的数字就会有更大的概率落到此区间内。只要随机数生成器产生的随机数分布性很好，在经过多次选择后，每个服务器被选中的次数比例接近其权重比例。比如，经过一万次选择后，服务器 A 被选中的次数大约为5000次，服务器 B 被选中的次数约为3000次，服务器 C 被选中的次数约为2000次。

假设现在随机数offset=7：

1. offset<5 is false，所以不在[0, 5)区间，将offset = offset - 5（offset=2）
2. offset<3 is true，所以处于[5, 8)区间，所以应该选用B服务器
   实现如下：

```java
public class WeightRandomV2 {
    public static String getServer() {
        int totalWeight = 0;
        boolean sameWeight = true; // 如果所有权重都相等，那么随机一个ip就好了
        Object[] weights = ServerIps.WEIGHT_LIST.values().toArray();
        for (int i = 0; i < weights.length; i++) {
            Integer weight = (Integer) weights[i];
            totalWeight += weight;
            if (sameWeight && i > 0 && !weight.equals(weights[i - 1])) {
                sameWeight = false;
            }
        }
        java.util.Random random = new java.util.Random();
        int randomPos = random.nextInt(totalWeight);
        if (!sameWeight) {
            for (String ip : ServerIps.WEIGHT_LIST.keySet()) {
                Integer value = ServerIps.WEIGHT_LIST.get(ip);
                if (randomPos < value) {
                    return ip;
                }
                randomPos = randomPos - value;
            }
        }
        //权重相同就随机返回一个IP
        return (String) ServerIps.WEIGHT_LIST.keySet().toArray()[new java.util.Random().nextInt(ServerIps.WEIGHT_LIST.size())];
    }
    public static void main(String[] args) {
        // 连续调用10次
        for (int i = 0; i < 10; i++) {
            System.out.println(getServer());
        } 
    }
}
```

这就是另外一种权重随机算法。

### 轮询算法-RoundRobinLoadBalance

简单的轮询算法很简单

```java
public class RoundRobin {
    // 当前循环的位置
    private static Integer pos = 0;
    public static String getServer() {
        String ip = null;
        // pos同步
        synchronized (pos) {
            if (pos >= ServerIps.LIST.size()) {
                pos = 0;
            }
            ip = ServerIps.LIST.get(pos);
            pos++;
        }
        return ip;
    }
    public static void main(String[] args) {
        // 连续调用10次
        for (int i = 0; i < 11; i++) {
            System.out.println(getServer());
        }
    }
}
```

```bash
运行结果：
192.168.0.1
192.168.0.2
192.168.0.3
192.168.0.4
192.168.0.5
192.168.0.6
192.168.0.7
192.168.0.8
192.168.0.9
192.168.0.10
192.168.0.1
```

这种算法很简单，也很**公平**，每台服务轮流来进行服务，但是有的机器性能好，所以**能者多劳**，和随机算法一下，加上权重这个维度之后，其中一种实现方法就是**复制法**，这里就不演示了，这种复制算法的缺点和随机算法的是一样的，比较消耗内存，那么自然就有其他实现方法。我下面来介绍一种算法：

这种算法需要加入一个概念：**调用编号**，比如第1次调用为1， 第2次调用为2， 第100次调用为100，调用编号是递增的，所以我们可以根据这个调用编号推算出服务器。

假设我们有三台服务器 servers = [A, B, C]，对应的权重为 weights = [ 2, 5, 1], 总权重为8，我们可以理解为有8台“服务器”，这是8台“不具有并发功能”，其中有2台为A，5台为B，1台为C，一次调用过来的时候，需要按顺序访问，比如有10次调用，那么服务器调用顺序为AABBBBBCAA，调用编号会越来越大，而服务器是固定的，所以需要把调用编号“缩小”，这里对调用编号进行**取余，除数为总权重和**，比如：

1. 1号调用，1%8=1；
2. 2号调用，2%8=2；
3. 3号调用，3%8=3；
4. 8号调用，8%8=0；
5. 9号调用，9%8=1；
6. 100号调用，100%8=4；
   我们发现调用编号可以被**缩小**为0-7之间的8个数字，问题是怎么根据这个8个数字找到对应的服务器呢？和我们随机算法类似，这里也可以把权重想象为一个坐标轴“0-----2-----7-----8”
7. 1号调用，1%8=1，offset = 1, offset <= 2 is true，取A；
8. 2号调用，2%8=2；offset = 2，offset <= 2 is true, 取A;
9. 3号调用，3%8=3；offset = 3, offset <= 2 is false, offset = offset - 2, offset = 1, offset <= 5，取B
10. 8号调用，8%8=0；offset = 0, 特殊情况，offset = 8，offset <= 2 is false, offset = offset - 2, offset = 6, offset  <= 5 is false, offset = offset - 5, offset = 1, offset <= 1 is true, 取C;
11. 9号调用，9%8=1；// ...
12. 100号调用，100%8=4； //...
    实现：
    模拟调用编号获取工具：

```java
public class Sequence {
    public static Integer num = 0;
    public static Integer getAndIncrement() {
        return ++num;
    }
}
```

```java
public class WeightRoundRobin {
    private static Integer pos = 0;
    public static String getServer() {
        int totalWeight = 0;
        boolean sameWeight = true; // 如果所有权重都相等，那么随机一个ip就好了
        Object[] weights = ServerIps.WEIGHT_LIST.values().toArray();
        for (int i = 0; i < weights.length; i++) {
            Integer weight = (Integer) weights[i];
            totalWeight += weight;
            if (sameWeight && i > 0 && !weight.equals(weights[i - 1])) {
                sameWeight = false;
            }
        }
        Integer sequenceNum = Sequence.getAndIncrement();
        Integer offset = sequenceNum % totalWeight;
        offset = offset == 0 ?  totalWeight : offset;
        if (!sameWeight) {
            for (String ip : ServerIps.WEIGHT_LIST.keySet()) {
                Integer weight = ServerIps.WEIGHT_LIST.get(ip);
                if (offset <= weight) {
                    return ip;
                }
                offset = offset - weight;
            }
        }
        //权重相同进行轮询
        String ip = null;
        synchronized (pos) {
            if (pos >= ServerIps.LIST.size()) {
                pos = 0;
            }
            ip = ServerIps.LIST.get(pos);
            pos++;
        }
        return ip;
    }
    public static void main(String[] args) {
        // 连续调用11次
        for (int i = 0; i < 11; i++) {
            System.out.println(getServer());
        }
    }
}
```

```bash
运行结果：
192.168.0.1
192.168.0.2
192.168.0.2
192.168.0.2
192.168.0.2
192.168.0.2
192.168.0.2
192.168.0.2
192.168.0.2
192.168.0.3
192.168.0.3
```

但是这种算法有一个缺点：一台服务器的权重特别大的时候，他需要连续的的处理请求，但是实际上我们想达到的效果是，对于100次请求，只要有100*8/50=16次就够了，这16次不一定要连续的访问，比如假设我们有三台服务器 servers = [A, B, C]，对应的权重为 weights = [5, 1, 1] , 总权重为7，那么上述这个算法的结果是：AAAAABC，那么如果能够是这么一个结果呢：AABACAA，把B和C平均插入到5个A中间，这样是比较均衡的了。

我们这里可以改成**平滑加权轮询。**

#### 平滑加权轮询

思路：每个服务器对应两个权重，分别为 weight 和 currentWeight。其中 weight 是固定的，currentWeight 会动态调整，初始值为0。当有新的请求进来时，遍历服务器列表，让它的 currentWeight 加上自身权重。遍历完成后，找到最大的 currentWeight，并将其减去权重总和，然后返回相应的服务器即可。

| 请求编号 | currentWeight 数组 (current_weight += weight) | 选择结果(max(currentWeight)) | 减去权重总和后的currentWeight 数组（max(currentWeight) -= sum(weight)) |
| -------- | --------------------------------------------- | ---------------------------- | ------------------------------------------------------------ |
| 1        | [5, 1, 1]                                     | A                            | [-2, 1, 1]                                                   |
| 2        | [3, 2, 2]                                     | A                            | [-4, 2, 2]                                                   |
| 3        | [1, 3, 3]                                     | B                            | [1, -4, 3]                                                   |
| 4        | [6, -3, 4]                                    | A                            | [-1, -3, 4]                                                  |
| 5        | [4, -2, 5]                                    | C                            | [4, -2, -2]                                                  |
| 6        | [9, -1, -1]                                   | A                            | [2, -1, -1]                                                  |
| 7        | [7, 0, 0]                                     | A                            | [0, 0, 0]                                                    |

如上，经过平滑性处理后，得到的服务器序列为 [A, A, B, A, C, A, A]，相比之前的序列 [A, A, A, A, A, B, C]，分布性要好一些。初始情况下 currentWeight = [0, 0, 0]，第7个请求处理完后，currentWeight 再次变为 [0, 0, 0]。

实现：

```java
// 增加一个Weight类，用来保存ip, weight（固定不变的原始权重）, currentweight（当前会变化的权重）
public class Weight {
    private String ip;
    private Integer weight;
    private Integer currentWeight;
    public Weight(String ip, Integer weight, Integer currentWeight) {
        this.ip = ip;
        this.weight = weight;
        this.currentWeight = currentWeight;
    }
    public String getIp() {
        return ip;
    }
    public void setIp(String ip) {
        this.ip = ip;
    }
    public Integer getWeight() {
        return weight;
    }
    public void setWeight(Integer weight) {
        this.weight = weight;
    }
    public Integer getCurrentWeight() {
        return currentWeight;
    }
    public void setCurrentWeight(Integer currentWeight) {
        this.currentWeight = currentWeight;
    }
}
```

```java
public class WeightRoundRobinV2 {
    private static Map<String, Weight> weightMap = new HashMap<String, Weight>();
    public static String getServer() {
        // java8
        int totalWeight = ServerIps.WEIGHT_LIST.values().stream().reduce(0, (w1, w2) -> w1+w2);
        // 初始化weightMap，初始时将currentWeight赋值为weight
        if (weightMap.isEmpty()) {
            ServerIps.WEIGHT_LIST.forEach((key, value) -> {
                weightMap.put(key, new Weight(key, value, value));
            });
        }
        // 找出currentWeight最大值
        Weight maxCurrentWeight = null;
        for (Weight weight : weightMap.values()) {
            if (maxCurrentWeight == null || weight.getCurrentWeight() > maxCurrentWeight.getCurrentWeight()) {
                maxCurrentWeight = weight;
            }
        }
        // 将maxCurrentWeight减去总权重和
        maxCurrentWeight.setCurrentWeight(maxCurrentWeight.getCurrentWeight() - totalWeight);
        // 所有的ip的currentWeight统一加上原始权重
        for (Weight weight : weightMap.values()) {
           weight.setCurrentWeight(weight.getCurrentWeight() + weight.getWeight());
        }
        // 返回maxCurrentWeight所对应的ip
        return maxCurrentWeight.getIp();
    }
    public static void main(String[] args) {
        // 连续调用10次
        for (int i = 0; i < 10; i++) {
            System.out.println(getServer());
        }
    }
}
```

将ServerIps里的数据简化为：

```java
WEIGHT_LIST.put("A", 5);
        WEIGHT_LIST.put("B", 1);
        WEIGHT_LIST.put("C", 1);
```

```bash
运行结果：
A
A
B
A
C
A
A
A
A
B
```

这就是**轮询算法**，一个循环很简单，但是真正在实际运用的过程中需要思考更多。

### 一致性哈希算法-ConsistentHashLoadBalance

服务器集群接收到一次请求调用时，可以根据根据请求的信息，比如客户端的ip地址，或请求路径与请求参数等信息进行哈希，可以得出一个哈希值，特点是对于相同的ip地址，或请求路径和请求参数哈希出来的值是一样的，只要能再增加一个算法，能够把这个哈希值映射成一个服务端ip地址，就可以使相同的请求（相同的ip地址，或请求路径和请求参数）落到同一服务器上。

因为客户端发起的请求情况是无穷无尽的（客户端地址不同，请求参数不同等等），所以对于的哈希值也是无穷大的，所以我们不可能把所有的哈希值都进行映射到服务端ip上，所以这里需要用到**哈希环**。如下图：

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201109202421.png)

- 哈希值如果需要ip1和ip2之间的，则应该选择ip2作为结果；
- 哈希值如果需要ip2和ip3之间的，则应该选择ip3作为结果；
- 哈希值如果需要ip3和ip4之间的，则应该选择ip4作为结果；
- 哈希值如果需要ip4和ip1之间的，则应该选择ip1作为结果；

上面这情况是比较均匀情况，如果出现ip4服务器不存在，那就是这样了：

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201109203012.png)
会发现，ip3和ip1直接的范围是比较大的，会有更多的请求落在ip1上，这是不“公平的”，解决这个问题需要加入**虚拟节点**，比如：

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201109203012.png)
其中ip2-1, ip3-1就是虚拟结点，并不能处理节点，而是等同于对应的ip2和ip3服务器。
实际上，这只是处理这种不均衡性的一种思路，实际上就算哈希环本身是均衡的，你也可以增加更多的虚拟节点来使这个环更加平滑，比如：

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201109203019.png)

这个彩环也是“公平的”，并且只有ip1,2,3,4是实际的服务器ip，其他的都是虚拟ip。
那么我们怎么来实现呢？
对于我们的服务端ip地址，我们肯定知道总共有多少个，需要多少个虚拟节点也有我们自己控制，虚拟节点越多则流量越均衡，另外哈希算法也是很关键的，哈希算法越散列流量也将越均衡。
实现：

```java
public class ConsistentHash {
    private static SortedMap<Integer, String> virtualNodes = new TreeMap<>();
    private static final int VIRTUAL_NODES = 160;
    static {
        // 对每个真实节点添加虚拟节点，虚拟节点会根据哈希算法进行散列
        for (String ip : ServerIps.LIST) {
            for (int i = 0; i < VIRTUAL_NODES; i++) {
                int hash = getHash(ip+"VN"+i);
                virtualNodes.put(hash, ip);
            }
        }
    }
    private static String getServer(String client) {
        int hash = getHash(client);
        // 得到大于该Hash值的排好序的Map
        SortedMap<Integer, String> subMap = virtualNodes.tailMap(hash);
        // 大于该hash值的第一个元素的位置
        Integer nodeIndex = subMap.firstKey();
        // 如果不存在大于该hash值的元素，则返回根节点
        if (nodeIndex == null) {
            nodeIndex = virtualNodes.firstKey();
        }
        // 返回对应的虚拟节点名称
        return subMap.get(nodeIndex);
    }
    private static int getHash(String str) {
        final int p = 16777619;
        int hash = (int) 2166136261L;
        for (int i = 0; i < str.length(); i++)
            hash = (hash ^ str.charAt(i)) * p;
        hash += hash << 13;
        hash ^= hash >> 7;
        hash += hash << 3;
        hash ^= hash >> 17;
        hash += hash << 5;
        // 如果算出来的值为负数则取其绝对值
        if (hash < 0)
            hash = Math.abs(hash);
        return hash;
    }
    public static void main(String[] args) {
        // 连续调用10次,随机10个client
        for (int i = 0; i < 10; i++) {
            System.out.println(getServer("client" + i));
        }
    }
}
```



### 最小活跃数算法-LeastActiveLoadBalance

前面几种方法主要目标是使服务端分配到的调用次数尽量均衡，但是实际情况是这样吗？调用次数相同，服务器的负载就均衡吗？当然不是，这里还要考虑每次调用的时间，而最小活跃数算法则是解决这种问题的。

活跃调用数越小，表明该服务提供者效率越高，单位时间内可处理更多的请求。此时应优先将请求分配给该服务提供者。在具体实现中，每个服务提供者对应一个活跃数。初始情况下，所有服务提供者活跃数均为0。每收到一个请求，活跃数加1，完成请求后则将活跃数减1。在服务运行一段时间后，性能好的服务提供者处理请求的速度更快，因此活跃数下降的也越快，此时这样的服务提供者能够优先获取到新的服务请求、这就是最小活跃数负载均衡算法的基本思想。除了最小活跃数，最小活跃数算法在实现上还引入了权重值。所以准确的来说，最小活跃数算法是基于加权最小活跃数算法实现的。举个例子说明一下，在一个服务提供者集群中，有两个性能优异的服务提供者。某一时刻它们的活跃数相同，则会根据它们的权重去分配请求，权重越大，获取到新请求的概率就越大。如果两个服务提供者权重相同，此时随机选择一个即可。

实现：

因为活跃数是需要服务器请求处理相关逻辑配合的，一次调用开始时活跃数+1，结束是活跃数-1，所以这里就不对这部分逻辑进行模拟了，直接使用一个map来进行模拟。

```java
// 服务器当前的活跃数
    public static final Map<String, Integer> ACTIVITY_LIST = new LinkedHashMap<String, Integer>();
    static {
        ACTIVITY_LIST.put("192.168.0.1", 2);
        ACTIVITY_LIST.put("192.168.0.2", 0);
        ACTIVITY_LIST.put("192.168.0.3", 1);
        ACTIVITY_LIST.put("192.168.0.4", 3);
        ACTIVITY_LIST.put("192.168.0.5", 0);
        ACTIVITY_LIST.put("192.168.0.6", 1);
        ACTIVITY_LIST.put("192.168.0.7", 4);
        ACTIVITY_LIST.put("192.168.0.8", 2);
        ACTIVITY_LIST.put("192.168.0.9", 7);
        ACTIVITY_LIST.put("192.168.0.10", 3);
    }
```

```java
public class LeastActive {
    private static String getServer() {
        // 找出当前活跃数最小的服务器
        Optional<Integer> minValue = ServerIps.ACTIVITY_LIST.values().stream().min(Comparator.naturalOrder());
        if (minValue.isPresent()) {
            List<String> minActivityIps = new ArrayList<>();
            ServerIps.ACTIVITY_LIST.forEach((ip, activity) -> {
                if (activity.equals(minValue.get())) {
                    minActivityIps.add(ip);
                }
            });
            // 最小活跃数的ip有多个，则根据权重来选，权重大的优先
            if (minActivityIps.size() > 1) {
                // 过滤出对应的ip和权重
                Map<String, Integer> weightList = new LinkedHashMap<String, Integer>();
                ServerIps.WEIGHT_LIST.forEach((ip, weight) -> {
                    if (minActivityIps.contains(ip)) {
                        weightList.put(ip, ServerIps.WEIGHT_LIST.get(ip));
                    }
                });
                int totalWeight = 0;
                boolean sameWeight = true; // 如果所有权重都相等，那么随机一个ip就好了
                Object[] weights = weightList.values().toArray();
                for (int i = 0; i < weights.length; i++) {
                    Integer weight = (Integer) weights[i];
                    totalWeight += weight;
                    if (sameWeight && i > 0 && !weight.equals(weights[i - 1])) {
                        sameWeight = false;
                    }
                }
                java.util.Random random = new java.util.Random();
                int randomPos = random.nextInt(totalWeight);
                if (!sameWeight) {
                    for (String ip : weightList.keySet()) {
                        Integer value = weightList.get(ip);
                        if (randomPos < value) {
                            return ip;
                        }
                        randomPos = randomPos - value;
                    }
                }
                return (String) weightList.keySet().toArray()[new java.util.Random().nextInt(weightList.size())];
            } else {
                return minActivityIps.get(0);
            }
        } else {
            return (String) ServerIps.WEIGHT_LIST.keySet().toArray()[new java.util.Random().nextInt(ServerIps.WEIGHT_LIST.size())];
        }
    }
    public static void main(String[] args) {
        // 连续调用10次,随机10个client
        for (int i = 0; i < 10; i++) {
            System.out.println(getServer());
        }
    }
}
```

这里因为不会对活跃数进行操作，所以结果是固定的（担任在随机权重的时候会随机，具体看源码实现，以及运行结果即可理解）。



# 灰度发布

针对集群部署的服务，需要发布新功能时，先发布其中一部分服务，使得集群服务中一部分是老版本服务逻辑，一部分是新版本服务逻辑，这种中间状态的发布状态叫做**灰度发布**。

举个简单例子，现在有一个User系统，有一个查询用户英文名的功能，该功能返回用户的英文名的全名(firstName+lastName)，User系统在生产环境中被部署到5台服务器组成的集群上。

现在需要修改该功能，使得该功能只需要返回用户英文名的firstName，功能修改完后，经过了完整的测试后，需要将该功能进行生产发布，如果将该新功能直接同时部署到5台服务器组成的集群上，那么如果该功能仍然有缺陷，那么讲影响所有用户，所以为了稳当，将该功能先部署到其中两台服务器上，这是集群中3台服务器是旧版本，2台服务器是老版本，并且还需要控制用户的流量，使得生产测试用户使用的是新版本（2台服务器），其他大部分用户使用的仍然是旧版本(另外3台服务器)。当生产测试用户测试过新版本后，如果新版本没有问题，则将另外三台也部署成新版本，移除流量控制，使得所有用户随机访问5台服务器。

![image.png](https://gitee.com/xudongyin/img/raw/master/img/20201119145337.png)

> 总结

从上面的例子可以看出，只要能实现上面的发布模型，都是灰度发布，不管使用方式或工具，特别需要注意的是灰度发布不仅限于后端程序的发布，前端、客户端发布时也都可以进行灰度发布。

